{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install gensim\n",
    "# !pip install git+https://github.com/boudinfl/pke.git\n",
    "# !python -m spacy download en\n",
    "# !pip install bert-extractive-summarizer --upgrade --force-reinstall\n",
    "# !pip install spacy==2.1.3 --upgrade --force-reinstall\n",
    "# !pip install -U nltk\n",
    "# !pip install -U pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/anhong-\n",
      "[nltk_data]     eun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/anhong-eun/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to /Users/anhong-\n",
      "[nltk_data]    |     eun/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/anhong-eun/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "https://pytorch.org/get-started/locally/<br>\n",
    "여기서 각자 pc환경에 맞게 다운로드하기\n",
    "꼭 환경에 맞아야 함 !!!!!!\n",
    "아직 gpu사용안하니까 쿠다는 안써용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## BERT Extractive Summarizer\n",
    "Summarize the text using BERT extractive summarizer. This is used to find important sentences and useful sentences from the complete text.<br>\n",
    "https://pypi.org/project/bert-extractive-summarizer/ <br>\n",
    "여기 읽으면 코드 이해가 쉬워용 파라미터 설명도 나와있슴니당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for error solve\n",
    "# pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nile River fed Egyptian civilization for hundreds of years. This soil was fertile, which means it was good for growing crops. As in many ancient societies, much of the knowledge of Egypt came about as priests studied the world to find ways to please the gods. The Old Kingdom started about 2575 B.C., when the Egyptian empire was gaining strength. This way, the pharaohs hoped to protect their bodies and treasures from robbers.\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "''' https://yurimkoo.github.io/python/2019/09/14/connect-db-with-python.html 참고 \n",
    "이 부분을 db에서 읽어오기 .. 이런식으로 읽어오면 될 듯 ..?\n",
    "pip install PyMySQL 설치후\n",
    "\n",
    "import pymysql\n",
    "\n",
    "juso_db = pymysql.connect(\n",
    "    user='root', \n",
    "    passwd='{설정한 비밀번호}', \n",
    "    host='127.0.0.1', \n",
    "    db='juso-db', \n",
    "    charset='utf8'\n",
    ")\n",
    "cursor = juso_db.cursor(pymysql.cursors.DictCursor)\n",
    "sql = \"SELECT * FROM `busan-jibun`;\"\n",
    "cursor.execute(sql)\n",
    "result = cursor.fetchall() \n",
    "'''\n",
    "f = open(\"egypt.txt\",\"r\")\n",
    "full_text = f.read()\n",
    "\n",
    "model = Summarizer()\n",
    "result = model(full_text, min_length=60, max_length = 500 , num_sentences = 5)\n",
    "# result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)\n",
    "\n",
    "summarized_text = ''.join(result)\n",
    "print (summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction\n",
    "Get important keywords from the text and filter those keywords that are present in the summarized text.<br>\n",
    "https://stackoverflow.com/questions/71838099/typeerror-candidate-selection-got-an-unexpected-keyword-argument-stoplist<br>\n",
    "참고해서 코드 수정함!(error 나서)<br>\n",
    "https://boudinfl.github.io/pke/build/html/unsupervised.html<br>\n",
    "여기 설명이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nile river', 'egyptians', 'egypt', 'nile', 'euphrates', 'tigris', 'old kingdom', 'crown', 'red land', 'longest river', 'narmer united upper', 'africa', 'mediterranean sea', 'hyksos', 'black land', 'new kingdom', 'ethiopia', 'middle kingdom', 'legend', 'lower egypt']\n",
      "['nile river', 'egyptians', 'egypt', 'nile', 'old kingdom', 'crown', 'red land', 'narmer united upper', 'africa', 'mediterranean sea', 'new kingdom', 'middle kingdom', 'legend', 'lower egypt']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import itertools\n",
    "import re\n",
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_nouns_multipartite(text):\n",
    "    out=[]\n",
    "\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    # extractor.load_document(input=text)\n",
    "\n",
    "    #    not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'PROPN'}\n",
    "    #pos = {'VERB', 'ADJ', 'NOUN'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "\n",
    "    extractor.load_document(input=text, stoplist=stoplist)\n",
    "    # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "\n",
    "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "    #    threshold/method parameters.\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                  threshold=0.75,\n",
    "                                  method='average')\n",
    "    keyphrases = extractor.get_n_best(n=20)\n",
    "\n",
    "    for key in keyphrases:\n",
    "        out.append(key[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "keywords = get_nouns_multipartite(full_text) \n",
    "print (keywords)\n",
    "filtered_keys=[]\n",
    "for keyword in keywords:\n",
    "    if keyword.lower() in summarized_text.lower():\n",
    "        filtered_keys.append(keyword)\n",
    "        \n",
    "print (filtered_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Mapping\n",
    "For each keyword get the sentences from the summarized text containing that keyword. <br>\n",
    "https://www.nltk.org/api/nltk.tokenize.html<br>\n",
    "https://flashtext.readthedocs.io/en/latest/keyword_processor.html<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for error solve\n",
    "# !pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nile river': ['The Nile River fed Egyptian civilization for hundreds of years.'], 'egyptians': ['Egyptians cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls.', 'The Nile provided so well for Egyptians that sometimes they had surpluses, or more goods than they needed.', 'For example, some ancient Egyptians learned to be scribes, people whose job was to write and keep records.', 'Egyptians believed that if a tomb was robbed, the person buried there could not have a happy afterlife.', 'This prosperity made life easier and provided greater opportunities for many Egyptians.', 'Egyptians created a government that divided the empire into 42 provinces.', 'Early Egyptians created a hieroglyphic system with about 700 characters.', 'Eventually, Egyptians equipped their reed boats with sails and oars.', 'Poorer Egyptians simply went to the roof to cool off after sunset.', 'Almost all Egyptians married when they were in their early teens.', 'Egyptians often painted walls white to reflect the blazing heat.'], 'egypt': ['Because the pharaoh was thought to be a god, government and religion were not separate in ancient Egypt.', 'In Egypt, people became slaves if they owed a debt, committed a crime, or were captured in war.', 'Children in Egypt played with toys such as dolls, animal figures, board games, and marbles.', 'The first rulers of Egypt were often buried in an underground tomb topped by mud brick.', 'Ancient Egypt had no money, so people exchanged goods that they grew or made.', 'It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.'], 'nile': ['The Nile provided so well for Egyptians that sometimes they had surpluses, or more goods than they needed.', 'Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in Africa.'], 'old kingdom': ['Historians divide ancient Egyptian dynasties into the Old Kingdom, the Middle Kingdom, and the New Kingdom.'], 'crown': ['It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.', 'It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.'], 'red land': ['The red land was the barren desert beyond the fertile region.'], 'narmer united upper': ['Legend says a king named Narmer united Upper and Lower Egypt.'], 'africa': ['Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in Africa.', 'It begins near the equator in Africa and flows north to the Mediterranean Sea.'], 'mediterranean sea': ['It begins near the equator in Africa and flows north to the Mediterranean Sea.'], 'new kingdom': ['During the New Kingdom, pharaohs began building more secret tombs in an area called the Valley of the Kings.', 'Historians divide ancient Egyptian dynasties into the Old Kingdom, the Middle Kingdom, and the New Kingdom.', 'Only a secret tomb built for a New Kingdom pharaoh was ever found with much of its treasure untouched.'], 'middle kingdom': ['Historians divide ancient Egyptian dynasties into the Old Kingdom, the Middle Kingdom, and the New Kingdom.', 'This period of Egyptian history is called the Middle Kingdom.'], 'legend': ['Legend says a king named Narmer united Upper and Lower Egypt.'], 'lower egypt': ['It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.', 'Legend says a king named Narmer united Upper and Lower Egypt.']}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences\n",
    "\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    return keyword_sentences\n",
    "\n",
    "sentences = tokenize_sentences(summarized_text)\n",
    "keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)\n",
    "        \n",
    "print (keyword_sentence_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate MCQ\n",
    "Get distractors (wrong answer choices) from Wordnet/Conceptnet and generate MCQ Questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for error solve\n",
    "# pip install -U wn==0.0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################################################\n",
      "NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \n",
      "#############################################################################\n",
      "\n",
      "\n",
      "1)  _______  cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls.\n",
      "\t a )   Egyptians\n",
      "\t b )   Angolan\n",
      "\t c )   Algerian\n",
      "\t d )   Bantu\n",
      "\n",
      "More options:  ['Basotho', 'Beninese', 'Berber', 'Black African', 'Burundian', 'Cameroonian', 'Carthaginian', 'Chadian', 'Chewa', 'Congolese', 'Djiboutian', 'Egyptian', 'Ethiopian', 'Eurafrican', 'Ewe', 'Fulani'] \n",
      "\n",
      "\n",
      "2) Because the pharaoh was thought to be a god, government and religion were not separate in ancient  _______ .\n",
      "\t a )   Egypt\n",
      "\t b )   Kuwait\n",
      "\t c )   Iraq\n",
      "\t d )   Saudi Arabia\n",
      "\n",
      "More options:  ['Jordan', 'Israel', 'Fertile Crescent', 'Turkey', 'Iran', 'Lebanon', 'Shari', 'Mauritania', 'Nigeria', 'Somali peninsula', 'Sierra Leone', 'Malawi', 'North Africa', 'Senegal', 'Mozambique', 'Lake Tanganyika'] \n",
      "\n",
      "\n",
      "3) The  _______  provided so well for Egyptians that sometimes they had surpluses, or more goods than they needed.\n",
      "\t a )   Entebbe\n",
      "\t b )   Nile\n",
      "\t c )   Gulu\n",
      "\t d )   Buganda\n",
      "\n",
      "More options:  ['Jinja', 'Lake Edward', 'kayunga', 'gulu', 'entebbe', 'Port Sudan', 'Omdurman', 'Darfur', 'Libyan Desert', 'Kordofan', 'Khartoum', 'Nubian Desert', 'Nyala', 'Aswan High Dam', 'Eastern Desert', 'Aswan'] \n",
      "\n",
      "\n",
      "4) It combined the red  _______  of Lower Egypt with the white  _______  of Upper Egypt.\n",
      "\t a )   Crown\n",
      "\t b )   Head\n",
      "\t c )   Capital\n",
      "\t d )   Masthead\n",
      "\n",
      "More options:  [] \n",
      "\n",
      "\n",
      "5) Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in  _______ .\n",
      "\t a )   Eurasia\n",
      "\t b )   Old World\n",
      "\t c )   Australia\n",
      "\t d )   Africa\n",
      "\n",
      "More options:  [] \n",
      "\n",
      "\n",
      "6)  _______  says a king named Narmer united Upper and Lower Egypt.\n",
      "\t a )   Fable\n",
      "\t b )   Legend\n",
      "\t c )   Adventure Story\n",
      "\t d )   Love Story\n",
      "\n",
      "More options:  ['Mystery', 'Myth', 'Parable', 'Plot', 'Short Story'] \n",
      "\n",
      "\n",
      "7) It combined the red Crown of  _______  with the white Crown of Upper Egypt.\n",
      "\t a )   Lower egypt\n",
      "\t b )   Aswan High Dam\n",
      "\t c )   Eastern Desert\n",
      "\t d )   Aswan\n",
      "\n",
      "More options:  ['Lake Nasser', 'Saqqara', 'Cairo', 'Luxor', 'Suez', 'Suez Canal'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "def get_wordsense(sent,word):\n",
    "    word= word.lower()\n",
    "    \n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    \n",
    "    \n",
    "    synsets = wn.synsets(word,'n')\n",
    "    if synsets:\n",
    "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list\n",
    "\n",
    "key_distractor_list = {}\n",
    "\n",
    "for keyword in keyword_sentence_mapping:\n",
    "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
    "    if wordsense:\n",
    "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
    "        if len(distractors) ==0:\n",
    "            distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "    else:\n",
    "        \n",
    "        distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "\n",
    "index = 1\n",
    "print (\"#############################################################################\")\n",
    "print (\"NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \")\n",
    "print (\"#############################################################################\\n\\n\")\n",
    "for each in key_distractor_list:\n",
    "    sentence = keyword_sentence_mapping[each][0]\n",
    "    pattern = re.compile(each, re.IGNORECASE)\n",
    "    output = pattern.sub( \" _______ \", sentence)\n",
    "    print (\"%s)\"%(index),output)\n",
    "    choices = [each.capitalize()] + key_distractor_list[each]\n",
    "    top4choices = choices[:4]\n",
    "    random.shuffle(top4choices)\n",
    "    optionchoices = ['a','b','c','d']\n",
    "    for idx,choice in enumerate(top4choices):\n",
    "        print (\"\\t\",optionchoices[idx],\")\",\" \",choice)\n",
    "    print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
    "    index = index + 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4071cf386d151c75a4b894873740270b107cb08369452670c76b33b6aa602a89"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
